# Servicios de IA - KnoBot

## ðŸ“¦ Estructura de Servicios AI

La infraestructura AI/RAG estÃ¡ implementada en `apps/ai/services/`:

### 1. **EmbeddingService** (`embedding_service.py`)
Genera embeddings (vectores) de texto usando Sentence Transformers.

**CaracterÃ­sticas:**
- Modelo multilingÃ¼e optimizado para espaÃ±ol: `paraphrase-multilingual-MiniLM-L12-v2`
- DimensiÃ³n de embeddings: 384
- MÃ©todos principales:
  - `encode(texts)` - Vectoriza uno o mÃ¡s textos
  - `encode_query(query)` - Vectoriza una consulta
  - `similarity(emb1, emb2)` - Calcula similitud coseno

**Uso:**
```python
from apps.ai.services import EmbeddingService

embedding_service = EmbeddingService()
embedding = embedding_service.encode("Â¿CÃ³mo configuro mi router?")
```

### 2. **VectorStore** (`vector_store.py`)
Gestiona la base de datos vectorial usando ChromaDB.

**CaracterÃ­sticas:**
- Almacenamiento persistente en `./chroma_db`
- BÃºsqueda por similitud coseno
- Soporte para metadatos y filtros
- MÃ©todos principales:
  - `add_documents()` - Agrega documentos
  - `search()` - Busca documentos similares
  - `update_document()` - Actualiza un documento
  - `delete_documents()` - Elimina documentos

**Uso:**
```python
from apps.ai.services import VectorStore

vector_store = VectorStore()
vector_store.add_documents(
    documents=["Texto del documento"],
    embeddings=[[0.1, 0.2, ...]],
    metadatas=[{"category": "soporte"}],
    ids=["doc-1"]
)
```

### 3. **RAGService** (`rag_service.py`)
Implementa Retrieval-Augmented Generation (RAG).

**CaracterÃ­sticas:**
- IndexaciÃ³n de documentos con embeddings
- BÃºsqueda semÃ¡ntica de contexto
- ConstrucciÃ³n de prompts con contexto
- MÃ©todos principales:
  - `index_document()` - Indexa un documento
  - `retrieve_context()` - Recupera contexto relevante
  - `build_context_prompt()` - Construye prompt con contexto

**Uso:**
```python
from apps.ai.services import RAGService

rag_service = RAGService()

# Indexar documento
rag_service.index_document(
    document_id="doc-123",
    content="El router se configura desde 192.168.1.1",
    metadata={"category": "configuracion"}
)

# Buscar contexto
context = rag_service.retrieve_context(
    query="Â¿CÃ³mo configuro mi router?",
    n_results=5
)
```

### 4. **ChatOrchestrator** (`chat_orchestrator.py`)
Orquestador principal que integra RAG con Gemini LLM.

**CaracterÃ­sticas:**
- IntegraciÃ³n con Google Gemini
- Soporte para RAG opcional
- GestiÃ³n de historial de conversaciÃ³n
- MÃ©todos principales:
  - `process_message()` - Procesa mensaje y genera respuesta
  - `create_conversation()` - Crea nueva conversaciÃ³n
  - `end_conversation()` - Finaliza conversaciÃ³n

**Uso:**
```python
from apps.ai.services import ChatOrchestrator

orchestrator = ChatOrchestrator()

# Procesar mensaje con RAG
response = orchestrator.process_message(
    conversation_id="conv-123",
    user_message="Â¿CÃ³mo reinicio mi router?",
    use_rag=True,
    n_context_docs=5
)

print(response['content'])  # Respuesta del asistente
print(response['context_used'])  # NÃºmero de documentos usados
```

### 5. **DocumentProcessor** (`document_processor.py`)
Procesa documentos de diferentes formatos.

**CaracterÃ­sticas:**
- Soporta PDF, DOCX, TXT
- ExtracciÃ³n de texto
- DivisiÃ³n en chunks con overlap
- MÃ©todos principales:
  - `process_document()` - Procesa documento segÃºn tipo
  - `chunk_text()` - Divide texto en chunks

**Uso:**
```python
from apps.ai.services import DocumentProcessor

processor = DocumentProcessor()

# Procesar PDF
text = processor.process_document(file_content, 'pdf')

# Dividir en chunks
chunks = processor.chunk_text(text, chunk_size=500, overlap=50)
```

## ðŸ”„ Flujo de Trabajo TÃ­pico

### 1. Indexar Base de Conocimiento

```python
from apps.ai.services import RAGService, DocumentProcessor
from apps.knowledge.models import Document

rag_service = RAGService()
processor = DocumentProcessor()

# Obtener documentos de la base de datos
documents = Document.objects.filter(is_active=True)

for doc in documents:
    # Procesar si es archivo
    if doc.file_url:
        text = processor.process_document(doc.file_content, doc.file_type)
    else:
        text = doc.content
    
    # Dividir en chunks si es muy largo
    if len(text) > 1000:
        chunks = processor.chunk_text(text)
        for i, chunk in enumerate(chunks):
            rag_service.index_document(
                document_id=f"{doc.id}-chunk-{i}",
                content=chunk,
                metadata={
                    "document_id": str(doc.id),
                    "chunk_index": i,
                    "category": doc.knowledge_base.category
                }
            )
    else:
        rag_service.index_document(
            document_id=str(doc.id),
            content=text,
            metadata={"category": doc.knowledge_base.category}
        )
```

### 2. Procesar Chat con RAG

```python
from apps.ai.services import ChatOrchestrator

orchestrator = ChatOrchestrator()

# Usuario envÃ­a mensaje
response = orchestrator.process_message(
    conversation_id="conv-uuid",
    user_message="Mi internet estÃ¡ lento, Â¿quÃ© puedo hacer?",
    use_rag=True,
    n_context_docs=5
)

# Respuesta incluye:
# - content: Texto de la respuesta
# - tokens_used: Tokens consumidos
# - context_used: Documentos de contexto utilizados
```

### 3. Chat sin RAG (solo conversaciÃ³n)

```python
# Para conversaciones generales sin necesidad de base de conocimiento
response = orchestrator.process_message(
    conversation_id="conv-uuid",
    user_message="Hola, Â¿cÃ³mo estÃ¡s?",
    use_rag=False
)
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

```env
# .env
GEMINI_API_KEY=your-gemini-api-key-here
```

### Modelos

- **Embeddings:** `paraphrase-multilingual-MiniLM-L12-v2` (384 dimensiones)
- **LLM:** Google Gemini Pro
- **Vector DB:** ChromaDB con DuckDB backend

## ðŸ“Š PrÃ³ximos Pasos

1. **Management Commands:**
   - `python manage.py index_knowledge` - Indexar toda la base de conocimiento
   - `python manage.py clear_vector_store` - Limpiar base vectorial

2. **Endpoints REST:**
   - `POST /api/chat/` - Enviar mensaje
   - `POST /api/knowledge/index/` - Indexar documento
   - `GET /api/knowledge/search/` - BÃºsqueda semÃ¡ntica

3. **Optimizaciones:**
   - Cache de embeddings
   - Batch processing
   - Async processing con Celery

## ðŸ§ª Testing

```python
# Test bÃ¡sico
from apps.ai.services import EmbeddingService, VectorStore, RAGService

# 1. Test embeddings
embedding_service = EmbeddingService()
emb = embedding_service.encode("Hola mundo")
assert emb.shape == (384,)

# 2. Test vector store
vector_store = VectorStore()
vector_store.add_documents(
    documents=["Test doc"],
    embeddings=[emb.tolist()],
    ids=["test-1"]
)
assert vector_store.count() > 0

# 3. Test RAG
rag_service = RAGService()
rag_service.index_document("test-2", "Documento de prueba")
results = rag_service.retrieve_context("prueba")
assert len(results) > 0
```
